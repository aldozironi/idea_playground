{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Feature Selection\n",
    "\n",
    "Source: [Zheng Alan Zhao and Huan Liu. _Spectral Feature Selection for Data Mining_. CRC Press, 2012](https://library.oapen.org/viewer/web/viewer.html?file=/bitstream/handle/20.500.12657/25274/1004820.pdf?sequence=1&isAllowed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* Data mining refers to extracting nuggets of knowledge from data.\n",
    "* Data has grown very large, both in number of observations and in dimensionality (number of features) in recent years.\n",
    "* **The Curse of Dimensionality:** The more features in your model the less well it will fit, and the more time it will take to train.\n",
    "* To avoid the curse of dimensionality, you need to reduce dimentions through what's known as dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Techniques\n",
    "\n",
    "* Dimensionality reduction techniques allow you to take out irrelevant (or barely relevant) information.\n",
    "* General process:\n",
    "    1. There is a scoring function, $r$, evaluates the renevance of the features given the data.\n",
    "    2. A matrix $W$ is chosen to select or reduce data to the number of desired features.\n",
    "    3. Features are then processed via the $W$ transform.\n",
    "* There are two main approaches to dimentionality reduction: feature selection and feature extraction.\n",
    "* It's sometimes useful to combine these approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "\n",
    "* Remove (ignore) features in your data to only keep those that are relevant enough.\n",
    "* Works by creating a **selection matrix**, $W$, that chooses which features to use.\n",
    "* Uses already existing features.\n",
    "* Really good when interpretability or knowledge extraction is required.\n",
    "* Usually not as much predictive power as feature extraction.\n",
    "\n",
    "**General formulation:**\n",
    "\n",
    "Assuming dataset $X \\in \\mathbb{R}^{n \\times m}$. The problem of feature selection can be formulated as\n",
    "\n",
    "$$\n",
    "\\max_W r\\left(\\hat{X}\\right) \\\\\n",
    "s.t. \\;\\; \\hat{X} = XW,\\\\\n",
    "W \\in \\{0,1\\}^{m \\times q},\\\\\n",
    "W^T \\mathbf{1}^{m \\times 1} =  \\mathbf{1}^{q \\times 1},\\\\\n",
    "\\left \\| W \\mathbf{1}^{q \\times 1} \\right \\|_0 = q\n",
    "$$\n",
    "\n",
    "Note that the conditions imply that each column in $W$ has exactly one 1 and the remaining values are zeros.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  7,  3],\n",
       "       [ 5,  6,  4],\n",
       "       [10,  9,  8]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 7, 3], [5, 6, 4], [10, 9, 8]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1, 0], [0, 0], [0, 1]])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3],\n",
       "       [ 5,  4],\n",
       "       [10,  8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are feature selection schemes for supervised, unsupervised, and semi-supervised learning.\n",
    "* There are three main strategies to evaluate feature relevance (compute $r$):\n",
    "    1. **Filter strategy:**\n",
    "        * **Basic idea:** choose the top $k$ features that best relate to the objective. This can mean:\n",
    "            1. Correlation with target variable,\n",
    "            2. Variance captured,\n",
    "            3. Any other measure that relates the feature to your objective.\n",
    "        * Generally (though not always) assumes feature relevances are independent.\n",
    "        * _Backward elimination_: Start with many features and drop the least relevant ones.\n",
    "        * _Forward elimination_: Start with few features and add the next most relevant ones.\n",
    "        * Heuristics have been developed for low-order interactions.\n",
    "        * Not biased towards a learner model, i.e. they stay the same for all models.\n",
    "        * Simple structures with fast solutions.\n",
    "        * High explainability, low performance.\n",
    "    2. **Wrapper strategy:**\n",
    "        * **Basic idea:** Features that provide better results in the model are more relevant.\n",
    "        * Starts with a predetermined algorithm, and the score $r$ is computed by model performance under the selection matrix $W$.\n",
    "        * Also uses forward and backward elimination.\n",
    "        * Heuristics have been developed to optimize the solution because models usually train non-linearly, making evaluation non-linear.\n",
    "    3. **Embedded strategy:**\n",
    "        * **Basic idea:** Let the model tell you what's relevant during training.\n",
    "        * Feature selection is incorporated in the training phase.\n",
    "        * Generally uses $L_1$ regularization.\n",
    "        * Usually more efficient than the wrapper strategy.\n",
    "        \n",
    "* Challenges of feature selection:\n",
    "    * **Redundant features:** Features that are relevant to the problem, while causing no ill-effect due to their removal.\n",
    "    * Several feature selection schemes aren't well prepared to deal with terabyte volumes of data.\n",
    "    * Most feature selection algorithms only work with structured data.\n",
    "    * Feature selection algorithms tend to fail when using small sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "* Combine already available information from features and condense them into a lower number of features.\n",
    "* Works by creating a **weight matrix**, $W'$, that combines the features.\n",
    "* Creates new features.\n",
    "* Creates features with more predictive power.\n",
    "* Looses interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Feature Selection\n",
    "\n",
    "* General and unified framework that encompasses algorithms for both feature selection and extraction.\n",
    "* Allows us to create novel techniques, specifically designed for a particular problem.\n",
    "* **Basic idea:** Identify features that associate similar values with samples of the same affiliation.\n",
    "* Measures relevance by the feature's capability of preserving pre-specified sample similarity by measuring their consistency with the spectrum of a matrix derived from a similarity matrix of the samples ($S$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Spectral Feature Selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Formulations for Spectral Feature Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def:\n",
    "\n",
    "Let $N$ denote the number of samples and $(x_i)_{i = 1}^{N}$ the vector of observed values. The **pairwise sample similarity** between $x_i$ and $x_j$, denoted by $s_{ij}$, is a non-negative funciton that measure of how similar two samples are.\n",
    "\n",
    "* For unlabeled data, we can define a $\\delta > 0$ and use the Gaussian radial basis function (RBF):\n",
    "\n",
    "$$\n",
    "s_{ij} = exp \\left( - \\; \\frac{ \\left\\| x_i - x_j \\right\\| ^2}{2 \\delta ^2} \\right)\n",
    "$$\n",
    "\n",
    "* For labeled data, if $n_l$ denotes the number of samples in class $l$, we can use:\n",
    "\n",
    "$$\n",
    "s_{ij} = \\left\\{\\begin{matrix}\n",
    "\\frac{1}{n_l}, & y_i = y_j = l\\\\ \n",
    "0 & \\mbox{otherwise}\n",
    "\\end{matrix}\\right. .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def:\n",
    "\n",
    "Let $N$ denote the number of samples. Then, given a pairwise similarity measure, $s_{ij}$, we define the **similarity matrix** as:\n",
    "\n",
    "$$\n",
    "S := S_{ij} = (s_{ij})\n",
    "$$\n",
    "\n",
    "Additionally, if $S$ has a positive semi-definite submatrix, it's called a **kernel matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Let $S$ be a similarity matrix of a sample. It is possible to construct a fully connected undirected graph, $G(V, E)$, where vertex $v_i$ corresponds to sample $x_i$ and $S$ is its adjacency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def:\n",
    "\n",
    "Let $G(V, E)$ be a graph with adjacency matrix $S=(s_{ij})$. The **degree matrix** of graph $G$ is the diagonal matrix constructed as\n",
    "\n",
    "$$\n",
    "D := D(i, j) = \\left\\{\\begin{matrix}\n",
    "d_i, & i = j\\\\ \n",
    "0 & \\mbox{otherwise}\n",
    "\\end{matrix}\\right. ,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "d_i = \\displaystyle\\sum_{k=1}^{N}s_{ik}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Using the notation from the previous definition, $d_i$ can be interpreted as an estimation of the density around sample $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def:\n",
    "\n",
    "Let $G$ be a graph with adjacency matrix $S$ and degree matrix $D$. The **Laplacian matrix** of $G$, denoted by $L$, is defined as:\n",
    "\n",
    "$$\n",
    "L := D - S\n",
    "$$\n",
    "\n",
    "Moreover, the **normalized Laplacian matrix** of $G$, denoted by $\\mathcal{L}$, is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} := D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
